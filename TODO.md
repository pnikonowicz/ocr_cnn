# TODO

## back propagation

* use logits, softmax to update weights and biases

### update weights

* compute the gradient of this mean loss with respect to each weight in the output layer
* continue the process (chain rule) going backward

## refactor neuron_test setup

* create BDD style to reuse setup for creating the ANN
