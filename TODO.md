# TODO

## back propagation

* use logits, softmax to update weights and biases

### update weights

* compute the gradient of this mean loss with respect to each weight in the output layer
* continue the process (chain rule) going backward
